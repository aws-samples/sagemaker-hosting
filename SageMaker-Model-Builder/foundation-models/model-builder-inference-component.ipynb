{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0d57a8",
   "metadata": {},
   "source": [
    "# ModelBuilder Inference Component\n",
    "\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5.2xl` with a 100GB EBS volume attached. If you are not using local mode, feel free to make use of a smaller CPU instance type / EBS volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e9a7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 sagemaker -U --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974aacb",
   "metadata": {},
   "source": [
    "# SageMaker Model Builder experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91159",
   "metadata": {},
   "source": [
    "In the new experience, we have introduced a few new constructs. Here we will focus on the following: \n",
    "\n",
    "1. ModelBuilder\n",
    "2. SchemaBuilder\n",
    "3. InferenceSpec\n",
    "\n",
    "In the following section, we will define these constructs and provide examples to elaborate on each one.\n",
    "\n",
    "4.1 ModelBuilder:\n",
    "\n",
    "ModelBuilder is a Python class that takes a framework model (such as XGBoost or PyTorch) or an Inference Spec (more details below) and converts them into a SageMaker deployable model. ModelBuilder provides a `build` function that generates the artifacts for deployment. The model artifact generated is specific to the model server, which is also customizable as one of the inputs.\n",
    "\n",
    "```python\n",
    "Class definition:\n",
    "\n",
    "class ModelBuilder(\n",
    "    model_path: str | None = '/tmp/sagemaker/model-builder/' + uuid.uuid1().hex,\n",
    "    role_arn: str | None = None,\n",
    "    sagemaker_session: Session | None = None,\n",
    "    name: str | None = 'model-name-' + uuid.uuid1().hex,\n",
    "    mode: Mode | None = Mode.SAGEMAKER_ENDPOINT,\n",
    "    shared_libs: List[str] = lambda : [],\n",
    "    dependencies: Dict[str, Any] | None = lambda : { \"auto\": False },\n",
    "    env_vars: Dict[str, str] | None = lambda : {},\n",
    "    log_level: int | None = logging.DEBUG,\n",
    "    content_type: str | None = None,\n",
    "    accept_type: str | None = None,\n",
    "    s3_model_data_url: str | None = None,\n",
    "    instance_type: str | None = \"ml.c5.xlarge\",\n",
    "    schema_builder: str | None = None,\n",
    "    model: Any | None = None,\n",
    "    inference_spec: InferenceSpec = None,\n",
    "    image_uri: str | None = None,\n",
    "    model_server: str | None = None\n",
    ")\n",
    "```\n",
    "Example:\n",
    "\n",
    "The above class file provide all the options for customization. However to deploy the framework model, the model builder just expects model, input, output and the role. \n",
    "\n",
    "```python\n",
    "model_builder = ModelBuilder(\n",
    "    model=model,  # Pass in the actual model object. It's \"predict\" method will be invoked in the endpoint.\n",
    "    schema_builder=SchemaBuilder(input, output), # Pass in a \"SchemaBuilder\" which will use the sample test input and output objects to infer the serialization needed.\n",
    "    role_arn=role, # Pass in the role arn or update intelligent defaults.\n",
    "    )\n",
    "```\n",
    "\n",
    "4.2 SchemaBuilder:\n",
    "\n",
    "The SchemaBuilder enables you to define the input and output for your endpoint. It allows the SchemaBuilder to generate the corresponding marshalling functions for serializing and deserializing the input and output. For further details, please consult the notebook or refer to the video.\n",
    "\n",
    "Class definition:\n",
    "```python\n",
    "class SchemaBuilder(\n",
    "    sample_input: Any,\n",
    "    sample_output: Any,\n",
    "    input_translator: CustomPayloadTranslator = None,\n",
    "    output_translator: CustomPayloadTranslator = None\n",
    ")\n",
    "```\n",
    "Example:\n",
    "\n",
    "The CustomPayloadTranslator class provides all the options for customization. However, for [common inference data format](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html), you can just provide the sample input/output for the SchemaBuilder.\n",
    "```python\n",
    "input = \"How is the demo going?\"\n",
    "output = \"Comment la d√©mo va-t-elle?\"\n",
    "schema = SchemaBuilder(input, output)\n",
    "```\n",
    "\n",
    "4.3 InferenceSpec\n",
    "\n",
    "In the case you want to specify custom function to load and invoke the model instead of the framework model function, then you can pass the inference spec with your implementation in `load` and `invoke` function. \n",
    "\n",
    "class definition:\n",
    "```python\n",
    "class InferenceSpec(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def load(self, model_dir: str):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def invoke(self, input_object: object, model: object):\n",
    "        pass\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "class MyInferenceSpec(InferenceSpec):\n",
    "    def load(self, model_dir: str):\n",
    "        return pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "        \n",
    "    def invoke(self, input, model):\n",
    "        return model(input)\n",
    "   \n",
    "inf_spec = MyInferenceSpec()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481b830-e1ae-459e-8549-32384c4bbe80",
   "metadata": {},
   "source": [
    "In this example, we are using ModelBuilder to deploy an Llama 2 model directly. You can use `Mode` to switch between local testing and deploying to a SageMaker Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d61a15-53e1-4687-a15c-69d6e3b695e3",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Local deployment\n",
    "\n",
    "Now we will use SageMaker ModelBuilder class to prepare the model for local and remote deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d643065-0f31-4da4-a472-2c4e208cfd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60004f1-97b7-4c63-b784-89515292f58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'llama2-7b') \n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3e702-deac-4890-8d76-7b2d5cb9b572",
   "metadata": {},
   "source": [
    "[Llama 2 is a gated model](https://huggingface.co/meta-llama/Llama-2-7b-hf) and requires access to be approved. Once approved you will need to pass your [Hugging Face access token](https://huggingface.co/docs/hub/security-tokens) as seen in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351efc2-cc4e-4a6e-ad0c-bf2c6ab87953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.serve import Mode\n",
    "import json\n",
    "\n",
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {}\n",
    "}\n",
    "\n",
    "sample_output = [\n",
    "    {\n",
    "        \"generated_text\": response\n",
    "    }\n",
    "]\n",
    "\n",
    "model_builder_1 = ModelBuilder(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path, #local path where artifacts will be saved\n",
    "    mode=Mode.LOCAL_CONTAINER, # The model will be deployed locally. Change to Mode.SAGEMAKER_ENDPOINT to deploy to a SageMaker endpoint. \n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"<YourHuggingFaceToken>\" # Llama 2 is a gated model and requires a Hugging Face Hub token. \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1a1d1-572c-436e-8b6a-d9378b1e3679",
   "metadata": {},
   "source": [
    "By default, ModelBuilder will use [TGI](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers) container as the underlying container for Hugging Face models. In case you would like to use the [LMI containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers), you can configure the ModelBuilder as follow:\n",
    "\n",
    "```python\n",
    "from sagemaker.serve import ModelServer\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path, #local path where artifacts will be saved\n",
    "    mode=Mode.LOCAL_CONTAINER, # The model will be deployed locally. Change to Mode.SAGEMAKER_ENDPOINT to deploy to a SageMaker endpoint. \n",
    "    model_server=ModelServer.DJL_SERVING,\n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"<YourHuggingFaceToken>\" # Llama 2 is a gated model and requires a Hugging Face Hub token. \n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e23e5d-c863-4528-a3b8-ec83cd6889e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1 = model_builder_1.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184f43-61b4-4299-b873-bc2cf6ff5fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_predictor = model_1.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569f5d8-0185-496a-bd8f-1a5835db8da2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inferred_sample_prompt = model_builder_1.schema_builder.sample_input\n",
    "inferred_sample_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef5548-2178-48c4-83e5-6e808be1ce30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = local_predictor.predict(inferred_sample_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e1f8d-3bec-4c82-a514-fa84daaf39ee",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Deploy to a SageMaker Endpoint - Inference Component\n",
    "\n",
    "Now that we have tested the model prediction locally, we can continue to deploy the model to a SageMaker endpoint using an Inference Component. Lama2-7b can fit on 1 GPU and as such we will create an Inference Component with 1 accelerator. We will also set the `NUM_SHARD` to `1` as there will only be 1 GPU available for the model to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2edb6f-341f-4c3d-98c4-3f642c9d4efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1.env[\"NUM_SHARD\"] = \"1\"\n",
    "model_1.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8eabf8-0563-4d3c-9b14-0f13785fdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import unique_name_from_base\n",
    "from sagemaker.enums import EndpointType\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "resource_requirements = ResourceRequirements(\n",
    "    requests={\n",
    "        \"num_accelerators\": 1, #llama2-7b can fit on 1 GPU\n",
    "        \"memory\": 1024,\n",
    "        \"copies\": 3, # We will deploy on a ml.g5.12xlarge which has 4 GPUs so lets have 3 copies and leave one slot for the next model\n",
    "    },\n",
    "    limits={},\n",
    ")\n",
    "\n",
    "model_1.model_name = unique_name_from_base(\"llama2-7b-ic\")\n",
    "\n",
    "predictor_1 = model_1.deploy(\n",
    "    instance_type=\"ml.g5.12xlarge\", # Here we set the instance type which has 4 GPUs. If we did not set the instance type Model Builder would auto detect the instance we are running on. \n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    endpoint_name=unique_name_from_base(\"model-builder-llama2-7b-ic-endpoint\"),\n",
    "    resources=resource_requirements,\n",
    "    role=execution_role,\n",
    "    endpoint_logging=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9f5fb-a605-458b-8a65-f3f985bf9477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predictor_1.predict(inferred_sample_prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc344d8d-1c64-4aa7-84a2-f0248dd6a7b5",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Deploy a second model to the SageMaker Endpoint - Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc75f31-bc5d-4ae6-ab96-43103370322c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {}\n",
    "}\n",
    "\n",
    "sample_output = [\n",
    "    {\n",
    "        \"generated_text\": response\n",
    "    }\n",
    "]\n",
    "\n",
    "model_builder_2 = ModelBuilder(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path,\n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"<YourHuggingFaceToken>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1621a8-c1df-4c27-be16-50d17bc9ecad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_2 = model_builder_2.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a11fe6-081e-4e39-9d06-f2f1d63ae4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_2.env[\"NUM_SHARD\"] = \"1\"\n",
    "model_2.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383575d-4173-4f9d-950b-fc3afaabfbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource_requirements = ResourceRequirements(\n",
    "    requests={\n",
    "        \"num_accelerators\": 1,\n",
    "        \"memory\": 1024,\n",
    "        \"copies\": 1,\n",
    "    },\n",
    "    limits={},\n",
    ")\n",
    "\n",
    "model_2.model_name = unique_name_from_base(\"lama2-7b-ic-2\")\n",
    "\n",
    "predictor_2 = model_2.deploy(\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    endpoint_name=predictor_1.endpoint,\n",
    "    resources=resource_requirements,\n",
    "    role=execution_role,\n",
    "    endpoint_logging=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ecfc7-2f3a-48b9-a074-921e2d803463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predictor_2.predict(inferred_sample_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8f0fe",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68053e21-a69d-4b1d-8a64-7d3f13e53cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_1.delete_predictor(wait=True)\n",
    "predictor_2.delete_predictor(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43e20c-c04c-445c-a43f-34ee13fad448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_1.delete_endpoint()\n",
    "try:\n",
    "    predictor_2.delete_endpoint()\n",
    "except Exception:\n",
    "    print(\"endpoint successfully deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
