{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2-7b Model with response streaming (using TGI container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN=\"<your Huggingface read token goes here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\"\n",
    ")\n",
    "\n",
    "role = get_execution_role()\n",
    "hf_model_id = \"meta-llama/Llama-2-7b-chat-hf\" # model id from huggingface.co/models\n",
    "model_name = hf_model_id.replace(\"/\",\"-\").replace(\".\",\"-\")\n",
    "endpoint_name = \"Llama-2-7b-chat-hf-endpoint\"\n",
    "instance_type = \"ml.g5.2xlarge\" # instance type to use for deployment\n",
    "number_of_gpus = 1 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 900 # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "      role=role,\n",
    "      image_uri=llm_image,\n",
    "      env={\n",
    "        'HF_MODEL_ID': hf_model_id,\n",
    "        'HUGGING_FACE_HUB_TOKEN': HUGGING_FACE_HUB_TOKEN,\n",
    "        'SM_NUM_GPUS': f\"{number_of_gpus}\"\n",
    "      },\n",
    "      name=model_name\n",
    "    )\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class StreamIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == 10:\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print(f\"Unknown event type: {chunk}\")\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name=\"Llama-2-7b-chat-hf-endpoint\"\n",
    "\n",
    "message = \"Write a short poem about Sydney in Australia\"\n",
    "\n",
    "system_message = \"You are a priate\"\n",
    "\n",
    "Roles = [\"<s>[INST]\", \"[\\INST]\"]\n",
    "\n",
    "prompt = f\"{Roles[0]} <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{message} {Roles[1]}\"\n",
    "\n",
    "smr = boto3.client(\"sagemaker-runtime\")\n",
    "special = False\n",
    "data = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"</s>\"],\n",
    "    },\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "res = smr.invoke_endpoint_with_response_stream(\n",
    "    Body=json.dumps(data),\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "text = \"\"\n",
    "for chunk in StreamIterator(res[\"Body\"]):\n",
    "    if chunk:\n",
    "        # print(chunk)\n",
    "        special = json.loads(chunk[5:])[\"token\"][\"special\"]\n",
    "        text += json.loads(chunk[5:])[\"token\"][\"text\"]\n",
    "        if not special:\n",
    "            # text += json.loads(chunk[5:])[\"token\"][\"text\"]\n",
    "            print(json.loads(chunk[5:])[\"token\"][\"text\"], end=\"\")\n",
    "\n",
    "prompt += text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
