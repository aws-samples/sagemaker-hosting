{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce03d08-6de8-474f-811f-98f643c7948b",
   "metadata": {},
   "source": [
    "## Deploy the Clip Interrogator on Amazon SageMaker\n",
    "\n",
    "The [clip interrogator](https://github.com/pharmapsychotic/clip-interrogator) is a prompt engineering tool that combines [CLIP](https://openai.com/blog/clip/) and [BLIP](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/) to optimize text prompts to match a given image. You can then use the resulting prompts with text-to-image models like [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-2-1) on Amazon SageMaker to create cool art!\n",
    "\n",
    "In this notebook, we demonstrate how to use SageMaker [large model inference container](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) to host the clip interrogator solution on Amazon SageMaker using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to the [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc931b8-6ab5-4c06-b103-997a53b1a5dc",
   "metadata": {},
   "source": [
    "### Setup for Model Deployment\n",
    "As a first step, we'll import the relevant libraries and configure several global variables such as the hosting image that will be used and the S3 location to store the relevant artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95351e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f8f6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a78eb19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model will be uploaded to ---- > s3://sagemaker-us-west-2-631450739534/model_clip_interrogator/\n"
     ]
    }
   ],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"clip_interrogator\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"model_clip_interrogator\"  # folder within bucket where code artifact will go\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775a2e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=sess.boto_session.region_name, version=\"0.22.1\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7f3ce-1c32-4256-a7a5-2d269cf1174f",
   "metadata": {},
   "source": [
    "### 1. Create SageMaker compatible model artifacts\n",
    "In order to prepare our model for deployment to a SageMaker Endpoint for hosting, we will need to prepare a few things for SageMaker and our container. We will use a local folder as the location of these files including **serving.properties** that defines parameters for the LMI container and **requirements.txt** to detail what dependies to install."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653f05-ebde-4da1-90e1-5b086a612ac3",
   "metadata": {},
   "source": [
    "In the **serving.properties** files define the the engine to use and model to host. Note the tensor_parallel_degree parameter which is set to a value of 1 in this scenario. In this example, we will use the `Salesforce/blip-image-captioning-large` and the `ViT-L-14/openai` models. Since both of the two models can fit on a sigle GPU we do not have to divide the model into multiple parts. In this case we will use a 'ml.g5.xlarge' instance which provides 1 GPU. Be careful not to specify a value larger than the instance provides or your deployment will fail. If you are choosing a bigger model and needs more GPU resources, please update the instance type and tensor parallel degree accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32859970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p clipinterrogator\n",
    "!cp -r data clipinterrogator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81838f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing clipinterrogator/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile clipinterrogator/serving.properties\n",
    "engine = Python\n",
    "option.tensor_parallel_degree = 1\n",
    "# option.s3url = {{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58782173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing clipinterrogator/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile clipinterrogator/requirements.txt\n",
    "Pillow\n",
    "requests\n",
    "safetensors\n",
    "tqdm\n",
    "open_clip_torch\n",
    "transformers>=4.27.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb7ca5-82fc-485a-aabd-08d7cded23bc",
   "metadata": {},
   "source": [
    "### 2. Create a model.py with custom inference code\n",
    "SageMaker allows you to bring your own script for inference. Here we create our **model.py** file with the appropriate code for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae948c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing clipinterrogator/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clipinterrogator/model.py\n",
    "import hashlib\n",
    "import math\n",
    "import numpy as np\n",
    "import os,io\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BlipForConditionalGeneration, Blip2ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "import open_clip\n",
    "\n",
    "\n",
    "from djl_python import Input, Output\n",
    "\n",
    "from safetensors.numpy import load_file, save_file\n",
    "\n",
    "CAPTION_MODELS = {\n",
    "    'blip-base': 'Salesforce/blip-image-captioning-base',   # 990MB\n",
    "    'blip-large': 'Salesforce/blip-image-captioning-large', # 1.9GB\n",
    "    'blip2-2.7b': 'Salesforce/blip2-opt-2.7b',              # 15.5GB\n",
    "    'blip2-flan-t5-xl': 'Salesforce/blip2-flan-t5-xl',      # 15.77GB\n",
    "    'git-large-coco': 'microsoft/git-large-coco',           # 1.58GB\n",
    "}\n",
    "\n",
    "\n",
    "CACHE_URL_BASE = 'https://huggingface.co/pharma/ci-preprocess/resolve/main/'\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class Config:\n",
    "    # models can optionally be passed in directly\n",
    "    caption_model = None\n",
    "    caption_processor = None\n",
    "    clip_model = None\n",
    "    clip_preprocess = None\n",
    "\n",
    "    # blip settings\n",
    "    caption_max_length: int = 32\n",
    "    caption_model_name: Optional[str] = 'blip-large' # use a key from CAPTION_MODELS or None\n",
    "    caption_offload: bool = False\n",
    "\n",
    "    # clip settings\n",
    "    clip_model_name: str = 'ViT-L-14/openai'\n",
    "    clip_model_path: Optional[str] = None\n",
    "    clip_offload: bool = False\n",
    "\n",
    "    # interrogator settings\n",
    "    cache_path: str = '/opt/ml/cache'   # path to store cached text embeddings\n",
    "    download_cache: bool = True # when true, cached embeds are downloaded from huggingface\n",
    "    chunk_size: int = 2048      # batch size for CLIP, use smaller for lower VRAM\n",
    "    data_path: str = os.path.join(os.path.dirname(__file__), 'data')\n",
    "    device: str = (\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    flavor_intermediate_count: int = 2048\n",
    "    quiet: bool = False # when quiet progress bars are not shown\n",
    "\n",
    "    # def apply_low_vram_defaults(self):\n",
    "    #     self.caption_model_name = 'blip-base'\n",
    "    #     self.caption_offload = True\n",
    "    #     self.clip_offload = True\n",
    "    #     self.chunk_size = 1024\n",
    "    #     self.flavor_intermediate_count = 1024\n",
    "\n",
    "class Interrogator():\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.dtype = torch.float16 if self.device == 'cuda' else torch.float32\n",
    "        self.caption_offloaded = True\n",
    "        self.clip_offloaded = True\n",
    "        self.load_caption_model()\n",
    "        self.load_clip_model()\n",
    "\n",
    "    def load_caption_model(self):\n",
    "        if self.config.caption_model is None and self.config.caption_model_name:\n",
    "            if not self.config.quiet:\n",
    "                print(f\"Loading caption model {self.config.caption_model_name}...\")\n",
    "\n",
    "            model_path = CAPTION_MODELS[self.config.caption_model_name]\n",
    "            if self.config.caption_model_name.startswith('git-'):\n",
    "                caption_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)\n",
    "            elif self.config.caption_model_name.startswith('blip2-'):\n",
    "                caption_model = Blip2ForConditionalGeneration.from_pretrained(model_path, torch_dtype=self.dtype)\n",
    "            else:\n",
    "                caption_model = BlipForConditionalGeneration.from_pretrained(model_path, torch_dtype=self.dtype)\n",
    "            self.caption_processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "            caption_model.eval()\n",
    "            if not self.config.caption_offload:\n",
    "                caption_model = caption_model.to(self.config.device)\n",
    "            self.caption_model = caption_model\n",
    "        else:\n",
    "            self.caption_model = self.config.caption_model\n",
    "            self.caption_processor = self.config.caption_processor\n",
    "\n",
    "    def load_clip_model(self):\n",
    "        start_time = time.time()\n",
    "        config = self.config\n",
    "\n",
    "        clip_model_name, clip_model_pretrained_name = config.clip_model_name.split('/', 2)\n",
    "\n",
    "        if config.clip_model is None:\n",
    "            if not config.quiet:\n",
    "                print(f\"Loading CLIP model {config.clip_model_name}...\")\n",
    "\n",
    "            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
    "                clip_model_name, \n",
    "                pretrained=clip_model_pretrained_name, \n",
    "                precision='fp16' if config.device == 'cuda' else 'fp32',\n",
    "                device=config.device,\n",
    "                jit=False,\n",
    "                cache_dir=config.clip_model_path\n",
    "            )\n",
    "            self.clip_model.eval()\n",
    "        else:\n",
    "            self.clip_model = config.clip_model\n",
    "            self.clip_preprocess = config.clip_preprocess\n",
    "        self.tokenize = open_clip.get_tokenizer(clip_model_name)\n",
    "\n",
    "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribbble', \n",
    "                 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', \n",
    "                 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
    "        trending_list = [site for site in sites]\n",
    "        trending_list.extend([\"trending on \"+site for site in sites])\n",
    "        trending_list.extend([\"featured on \"+site for site in sites])\n",
    "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
    "\n",
    "        raw_artists = load_list(config.data_path, 'artists.txt')\n",
    "        artists = [f\"by {a}\" for a in raw_artists]\n",
    "        artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
    "\n",
    "        self._prepare_clip()\n",
    "        self.artists = LabelTable(artists, \"artists\", self)\n",
    "        self.flavors = LabelTable(load_list(config.data_path, 'flavors.txt'), \"flavors\", self)\n",
    "        self.mediums = LabelTable(load_list(config.data_path, 'mediums.txt'), \"mediums\", self)\n",
    "        self.movements = LabelTable(load_list(config.data_path, 'movements.txt'), \"movements\", self)\n",
    "        self.trendings = LabelTable(trending_list, \"trendings\", self)\n",
    "        self.negative = LabelTable(load_list(config.data_path, 'negative.txt'), \"negative\", self)\n",
    "\n",
    "        end_time = time.time()\n",
    "        if not config.quiet:\n",
    "            print(f\"Loaded CLIP model and data in {end_time-start_time:.2f} seconds.\")\n",
    "\n",
    "    def chain(\n",
    "        self, \n",
    "        image_features: torch.Tensor, \n",
    "        phrases: List[str], \n",
    "        best_prompt: str=\"\", \n",
    "        best_sim: float=0, \n",
    "        min_count: int=8,\n",
    "        max_count: int=32, \n",
    "        desc=\"Chaining\", \n",
    "        reverse: bool=False\n",
    "    ) -> str:\n",
    "        self._prepare_clip()\n",
    "\n",
    "        phrases = set(phrases)\n",
    "        if not best_prompt:\n",
    "            best_prompt = self.rank_top(image_features, [f for f in phrases], reverse=reverse)\n",
    "            best_sim = self.similarity(image_features, best_prompt)\n",
    "            phrases.remove(best_prompt)\n",
    "        curr_prompt, curr_sim = best_prompt, best_sim\n",
    "        \n",
    "        def check(addition: str, idx: int) -> bool:\n",
    "            nonlocal best_prompt, best_sim, curr_prompt, curr_sim\n",
    "            prompt = curr_prompt + \", \" + addition\n",
    "            sim = self.similarity(image_features, prompt)\n",
    "            if reverse:\n",
    "                sim = -sim\n",
    "            \n",
    "            if sim > best_sim:\n",
    "                best_prompt, best_sim = prompt, sim\n",
    "            if sim > curr_sim or idx < min_count:\n",
    "                curr_prompt, curr_sim = prompt, sim\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        for idx in tqdm(range(max_count), desc=desc, disable=self.config.quiet):\n",
    "            best = self.rank_top(image_features, [f\"{curr_prompt}, {f}\" for f in phrases], reverse=reverse)\n",
    "            flave = best[len(curr_prompt)+2:]\n",
    "            if not check(flave, idx):\n",
    "                break\n",
    "            if _prompt_at_max_len(curr_prompt, self.tokenize):\n",
    "                break\n",
    "            phrases.remove(flave)\n",
    "\n",
    "        return best_prompt\n",
    "\n",
    "    def generate_caption(self, pil_image: Image) -> str:\n",
    "        assert self.caption_model is not None, \"No caption model loaded.\"\n",
    "        self._prepare_caption()\n",
    "        inputs = self.caption_processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n",
    "        if not self.config.caption_model_name.startswith('git-'):\n",
    "            inputs = inputs.to(self.dtype)\n",
    "        tokens = self.caption_model.generate(**inputs, max_new_tokens=self.config.caption_max_length)\n",
    "        return self.caption_processor.batch_decode(tokens, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    def image_to_features(self, image: Image) -> torch.Tensor:\n",
    "        self._prepare_clip()\n",
    "        images = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = self.clip_model.encode_image(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    def interrogate_classic(self, image: Image, max_flavors: int=3, caption: Optional[str]=None) -> str:\n",
    "        \"\"\"Classic mode creates a prompt in a standard format first describing the image, \n",
    "        then listing the artist, trending, movement, and flavor text modifiers.\"\"\"\n",
    "        caption = caption or self.generate_caption(image)\n",
    "        image_features = self.image_to_features(image)\n",
    "\n",
    "        medium = self.mediums.rank(image_features, 1)[0]\n",
    "        artist = self.artists.rank(image_features, 1)[0]\n",
    "        trending = self.trendings.rank(image_features, 1)[0]\n",
    "        movement = self.movements.rank(image_features, 1)[0]\n",
    "        flaves = \", \".join(self.flavors.rank(image_features, max_flavors))\n",
    "\n",
    "        if caption.startswith(medium):\n",
    "            prompt = f\"{caption} {artist}, {trending}, {movement}, {flaves}\"\n",
    "        else:\n",
    "            prompt = f\"{caption}, {medium} {artist}, {trending}, {movement}, {flaves}\"\n",
    "\n",
    "        return _truncate_to_fit(prompt, self.tokenize)\n",
    "\n",
    "    def interrogate_fast(self, image: Image, max_flavors: int=32, caption: Optional[str]=None) -> str:\n",
    "        \"\"\"Fast mode simply adds the top ranked terms after a caption. It generally results in \n",
    "        better similarity between generated prompt and image than classic mode, but the prompts\n",
    "        are less readable.\"\"\"\n",
    "        caption = caption or self.generate_caption(image)\n",
    "        image_features = self.image_to_features(image)\n",
    "        merged = _merge_tables([self.artists, self.flavors, self.mediums, self.movements, self.trendings], self)\n",
    "        tops = merged.rank(image_features, max_flavors)\n",
    "        return _truncate_to_fit(caption + \", \" + \", \".join(tops), self.tokenize)\n",
    "\n",
    "    def interrogate_negative(self, image: Image, max_flavors: int = 32) -> str:\n",
    "        \"\"\"Negative mode chains together the most dissimilar terms to the image. It can be used\n",
    "        to help build a negative prompt to pair with the regular positive prompt and often \n",
    "        improve the results of generated images particularly with Stable Diffusion 2.\"\"\"\n",
    "        image_features = self.image_to_features(image)\n",
    "        flaves = self.flavors.rank(image_features, self.config.flavor_intermediate_count, reverse=True)\n",
    "        flaves = flaves + self.negative.labels\n",
    "        return self.chain(image_features, flaves, max_count=max_flavors, reverse=True, desc=\"Negative chain\")\n",
    "\n",
    "    def interrogate(self, image, params, caption: Optional[str]=None) -> str:\n",
    "\n",
    "        image = image\n",
    "        params = params\n",
    "        if \"max_flavors\" in params.keys():\n",
    "            max_flavors = params.pop(\"max_flavors\")\n",
    "        else:\n",
    "            max_flavors = 32\n",
    "            \n",
    "        if \"min_flavors\" in params.keys():\n",
    "            min_flavors = params.pop(\"min_flavors\")\n",
    "        else:\n",
    "            min_flavors = 8\n",
    "        caption = caption or self.generate_caption(image)\n",
    "        image_features = self.image_to_features(image)\n",
    "\n",
    "        merged = _merge_tables([self.artists, self.flavors, self.mediums, self.movements, self.trendings], self)\n",
    "        flaves = merged.rank(image_features, self.config.flavor_intermediate_count)\n",
    "        best_prompt, best_sim = caption, self.similarity(image_features, caption)\n",
    "        best_prompt = self.chain(image_features, flaves, best_prompt, best_sim, min_count=min_flavors, max_count=max_flavors, desc=\"Flavor chain\")\n",
    "\n",
    "        fast_prompt = self.interrogate_fast(image, max_flavors, caption=caption)\n",
    "        classic_prompt = self.interrogate_classic(image, max_flavors, caption=caption)\n",
    "        candidates = [caption, classic_prompt, fast_prompt, best_prompt]\n",
    "        response = candidates[np.argmax(self.similarities(image_features, candidates))]\n",
    "        return Output().add_as_json({\"outputs\": response})\n",
    "\n",
    "    def rank_top(self, image_features: torch.Tensor, text_array: List[str], reverse: bool=False) -> str:\n",
    "        self._prepare_clip()\n",
    "        text_tokens = self.tokenize([text for text in text_array]).to(self.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features @ image_features.T\n",
    "            if reverse:\n",
    "                similarity = -similarity\n",
    "        return text_array[similarity.argmax().item()]\n",
    "\n",
    "    def similarity(self, image_features: torch.Tensor, text: str) -> float:\n",
    "        self._prepare_clip()\n",
    "        text_tokens = self.tokenize([text]).to(self.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features @ image_features.T\n",
    "        return similarity[0][0].item()\n",
    "\n",
    "    def similarities(self, image_features: torch.Tensor, text_array: List[str]) -> List[float]:\n",
    "        self._prepare_clip()\n",
    "        text_tokens = self.tokenize([text for text in text_array]).to(self.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = text_features @ image_features.T\n",
    "        return similarity.T[0].tolist()\n",
    "\n",
    "    def _prepare_caption(self):\n",
    "        if self.config.clip_offload and not self.clip_offloaded:\n",
    "            self.clip_model = self.clip_model.to('cpu')\n",
    "            self.clip_offloaded = True\n",
    "        if self.caption_offloaded:\n",
    "            self.caption_model = self.caption_model.to(self.device)\n",
    "            self.caption_offloaded = False\n",
    "\n",
    "    def _prepare_clip(self):\n",
    "        if self.config.caption_offload and not self.caption_offloaded:\n",
    "            self.caption_model = self.caption_model.to('cpu')\n",
    "            self.caption_offloaded = True\n",
    "        if self.clip_offloaded:\n",
    "            self.clip_model = self.clip_model.to(self.device)\n",
    "            self.clip_offloaded = False\n",
    "\n",
    "\n",
    "class LabelTable():\n",
    "    def __init__(self, labels:List[str], desc:str, ci: Interrogator):\n",
    "        clip_model, config = ci.clip_model, ci.config\n",
    "        self.chunk_size = config.chunk_size\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.embeds = []\n",
    "        self.labels = labels\n",
    "        self.tokenize = ci.tokenize\n",
    "\n",
    "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
    "        sanitized_name = self.config.clip_model_name.replace('/', '_').replace('@', '_')\n",
    "        self._load_cached(desc, hash, sanitized_name)\n",
    "\n",
    "        if len(self.labels) != len(self.embeds):\n",
    "            self.embeds = []\n",
    "            chunks = np.array_split(self.labels, max(1, len(self.labels)/config.chunk_size))\n",
    "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None, disable=self.config.quiet):\n",
    "                text_tokens = self.tokenize(chunk).to(self.device)\n",
    "                with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                    text_features = clip_model.encode_text(text_tokens)\n",
    "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "                    text_features = text_features.half().cpu().numpy()\n",
    "                for i in range(text_features.shape[0]):\n",
    "                    self.embeds.append(text_features[i])\n",
    "\n",
    "            if desc and self.config.cache_path:\n",
    "                os.makedirs(self.config.cache_path, exist_ok=True)\n",
    "                cache_filepath = os.path.join(self.config.cache_path, f\"{sanitized_name}_{desc}.safetensors\")\n",
    "                tensors = {\n",
    "                    \"embeds\": np.stack(self.embeds),\n",
    "                    \"hash\": np.array([ord(c) for c in hash], dtype=np.int8)\n",
    "                }\n",
    "                save_file(tensors, cache_filepath)\n",
    "\n",
    "        if self.device == 'cpu' or self.device == torch.device('cpu'):\n",
    "            self.embeds = [e.astype(np.float32) for e in self.embeds]\n",
    "\n",
    "    def _load_cached(self, desc:str, hash:str, sanitized_name:str) -> bool:\n",
    "        if self.config.cache_path is None or desc is None:\n",
    "            return False\n",
    "\n",
    "        cached_safetensors = os.path.join(self.config.cache_path, f\"{sanitized_name}_{desc}.safetensors\")\n",
    "\n",
    "        if self.config.download_cache and not os.path.exists(cached_safetensors):\n",
    "            download_url = CACHE_URL_BASE + f\"{sanitized_name}_{desc}.safetensors\"\n",
    "            try:\n",
    "                os.makedirs(self.config.cache_path, exist_ok=True)\n",
    "                _download_file(download_url, cached_safetensors, quiet=self.config.quiet)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {download_url}\")\n",
    "                print(e)\n",
    "                return False                \n",
    "\n",
    "        if os.path.exists(cached_safetensors):\n",
    "            try:\n",
    "                tensors = load_file(cached_safetensors)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {cached_safetensors}\")\n",
    "                print(e)\n",
    "                return False\n",
    "            if 'hash' in tensors and 'embeds' in tensors:\n",
    "                if np.array_equal(tensors['hash'], np.array([ord(c) for c in hash], dtype=np.int8)):\n",
    "                    self.embeds = tensors['embeds']\n",
    "                    if len(self.embeds.shape) == 2:\n",
    "                        self.embeds = [self.embeds[i] for i in range(self.embeds.shape[0])]\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def _rank(self, image_features: torch.Tensor, text_embeds: torch.Tensor, top_count: int=1, reverse: bool=False) -> str:\n",
    "        top_count = min(top_count, len(text_embeds))\n",
    "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).to(self.device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            similarity = image_features @ text_embeds.T\n",
    "            if reverse:\n",
    "                similarity = -similarity\n",
    "        _, top_labels = similarity.float().cpu().topk(top_count, dim=-1)\n",
    "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
    "\n",
    "    def rank(self, image_features: torch.Tensor, top_count: int=1, reverse: bool=False) -> List[str]:\n",
    "        if len(self.labels) <= self.chunk_size:\n",
    "            tops = self._rank(image_features, self.embeds, top_count=top_count, reverse=reverse)\n",
    "            return [self.labels[i] for i in tops]\n",
    "\n",
    "        num_chunks = int(math.ceil(len(self.labels)/self.chunk_size))\n",
    "        keep_per_chunk = int(self.chunk_size / num_chunks)\n",
    "\n",
    "        top_labels, top_embeds = [], []\n",
    "        for chunk_idx in tqdm(range(num_chunks), disable=self.config.quiet):\n",
    "            start = chunk_idx*self.chunk_size\n",
    "            stop = min(start+self.chunk_size, len(self.embeds))\n",
    "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk, reverse=reverse)\n",
    "            top_labels.extend([self.labels[start+i] for i in tops])\n",
    "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
    "\n",
    "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
    "        return [top_labels[i] for i in tops]\n",
    "\n",
    "\n",
    "def _download_file(url: str, filepath: str, chunk_size: int = 4*1024*1024, quiet: bool = False):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code != 200:\n",
    "        return\n",
    "\n",
    "    file_size = int(r.headers.get(\"Content-Length\", 0))\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    progress = tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=filename, disable=quiet)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                progress.update(len(chunk))\n",
    "    progress.close()\n",
    "\n",
    "def _merge_tables(tables: List[LabelTable], ci: Interrogator) -> LabelTable:\n",
    "    m = LabelTable([], None, ci)\n",
    "    for table in tables:\n",
    "        m.labels.extend(table.labels)\n",
    "        m.embeds.extend(table.embeds)\n",
    "    return m\n",
    "\n",
    "def _prompt_at_max_len(text: str, tokenize) -> bool:\n",
    "    tokens = tokenize([text])\n",
    "    return tokens[0][-1] != 0\n",
    "\n",
    "def _truncate_to_fit(text: str, tokenize) -> str:\n",
    "    parts = text.split(', ')\n",
    "    new_text = parts[0]\n",
    "    for part in parts[1:]:\n",
    "        if _prompt_at_max_len(new_text + part, tokenize):\n",
    "            break\n",
    "        new_text += ', ' + part\n",
    "    return new_text\n",
    "\n",
    "def list_caption_models() -> List[str]:\n",
    "    return list(CAPTION_MODELS.keys())\n",
    "\n",
    "def list_clip_models() -> List[str]:\n",
    "    return ['/'.join(x) for x in open_clip.list_pretrained()]\n",
    "\n",
    "def load_list(data_path: str, filename: Optional[str] = None) -> List[str]:\n",
    "    \"\"\"Load a list of strings from a file.\"\"\"\n",
    "    if filename is not None:\n",
    "        data_path = os.path.join(data_path, filename)\n",
    "    with open(data_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        items = [line.strip() for line in f.readlines()]\n",
    "    return items\n",
    "\n",
    "with open('./model_name.json', 'rb') as openfile:\n",
    "    json_object = json.load(openfile)\n",
    "    \n",
    "caption_model_name = json_object.pop('caption_model_name')\n",
    "clip_model_name = json_object.pop('clip_model_name')\n",
    "config = Config()\n",
    "_service = Interrogator(config)\n",
    "\n",
    "def handle(inputs: Input) -> Optional[Output]:\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "\n",
    "    img = data[\"inputs\"]\n",
    "    imgdata = base64.b64decode(img)\n",
    "    image = Image.open(io.BytesIO(imgdata))\n",
    "\n",
    "    params = data[\"parameters\"]\n",
    "\n",
    "    if \"max_length\" in params.keys():\n",
    "        config.caption_max_length = params.pop(\"max_length\")\n",
    "\n",
    "\n",
    "    return _service.interrogate(image, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370fce0a-a77a-4265-9f40-b7f769eeca3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    \"caption_model_name\":'blip-large', #@param [\"blip-base\", \"blip-large\", \"git-large-coco\"]\n",
    "    \"clip_model_name\":'ViT-L-14/openai' #@param [\"ViT-L-14/openai\", \"ViT-H-14/laion2b_s32b_b79k\"]\n",
    "}\n",
    "with open(\"clipinterrogator/model_name.json\",'w') as file:\n",
    "    json.dump(model_names, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bdc5c-c29d-4a51-aeab-776f7df11cfb",
   "metadata": {},
   "source": [
    "### 3. Create the Tarball and then upload to S3 location\n",
    "Next, we will package our artifacts as `*.tar.gz` files for uploading to S3 for SageMaker to use for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc6b350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipinterrogator/\n",
      "clipinterrogator/model.py\n",
      "clipinterrogator/requirements.txt\n",
      "clipinterrogator/data/\n",
      "clipinterrogator/data/artists.txt\n",
      "clipinterrogator/data/movements.txt\n",
      "clipinterrogator/data/mediums.txt\n",
      "clipinterrogator/data/negative.txt\n",
      "clipinterrogator/data/flavors.txt\n",
      "clipinterrogator/model_name.json\n",
      "clipinterrogator/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tar czvf model.tar.gz clipinterrogator/\n",
    "rm -rf clipinterrogator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a63a8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-631450739534/clip_interrogator/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546b915-ec8f-48c7-883d-ff8ab647cbf9",
   "metadata": {},
   "source": [
    "### 4. Create the SageMaker Model and SageMaker endpoint\n",
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker endpoint using the high-level [python sdk](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a846991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(\"clip-interrogator\")\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_code_artifact,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c797519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c45ae3-8daf-4928-9ede-e3353257bff3",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "Once the endpoint is deployed, we can invoke the endpoint to test the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf5dfb85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=model_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2eeb68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 ms, sys: 212 Âµs, total: 17.7 ms\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'outputs': 'a close up of a plate with a croissant on it, pexels contest winner, wlop : :, anisotropic filtering, featured on artsation, interconnections, anime food, stylized border, picture of a loft in morning, foodphoto'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "file_name=\"croissant.jpeg\"  \n",
    "with open(file_name, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "payload = base64.b64encode(payload).decode()\n",
    "\n",
    "\n",
    "predictor.predict(\n",
    "                { \n",
    "                    \"inputs\" : payload, \n",
    "                    \"parameters\": { \n",
    "                        \"max_length\": 50,\n",
    "                        \"max_flavors\": 32,\n",
    "                    },\n",
    "                }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9f81c-f762-43a7-9c46-cf26f2455b48",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceced14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfc817-e9f9-434f-8a05-194f6e5f30d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
